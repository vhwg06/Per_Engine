# Implementation Plan: Evaluate Performance Orchestration Use Case

**Branch**: `002-evaluate-performance` | **Date**: January 15, 2026 | **Spec**: [spec.md](spec.md)  
**Input**: Feature specification from `/specs/002-evaluate-performance/spec.md`

## Summary

Implement the Evaluate Performance use case as an application-layer orchestration service that coordinates the Metrics, Profile, and Evaluation domains to produce deterministic, immutable EvaluationResult objects. This use case is **purely orchestration** ‚Äî it does not calculate metrics, evaluate rules in detail, or persist data. It translates domain inputs into deterministic evaluation workflows and ensures idempotency, traceability, and completeness transparency.

**Technical Approach**: 
- Clean Architecture positioned in **Application layer** with dependency flow: Application ‚Üí Domain Ports (no Infrastructure)
- Uses existing domain ports for Metrics, Profile, and Evaluation domains
- Determinism guaranteed through:
  - Sorted/ordered iteration over all inputs (rules, metrics, profiles)
  - Immutable result objects with computed fingerprints
  - Seeded deterministic algorithms for fingerprint generation
- No persistence, storage, or infrastructure dependencies in this layer

---

## Technical Context

**Language/Version**: C# 13 (.NET 10 LTS)  
**Primary Dependencies**: 
- .NET 10 base libraries
- Existing domain packages: `PerformanceEngine.Metrics.Domain`, `PerformanceEngine.Profile.Domain`, `PerformanceEngine.Evaluation.Domain`
- xUnit 2.8+ (testing framework for orchestration tests)
- FluentAssertions (test readability)

**Storage**: N/A (orchestration layer does not persist; data flows through, not stored)  
**Testing**: xUnit (unit tests), integration tests with domain test doubles  
**Target Platform**: Linux server (.NET 10 cross-platform), container-ready  
**Project Type**: Single application library (PerformanceEngine.Application package)  
**Performance Goals**: 
- Orchestration overhead < 10% of total evaluation time (target: < 50ms overhead for typical workload)
- Support 1000+ metrics and 100+ rules without degradation
- No GC pressure from orchestration layer (immutable result objects, minimal allocations)

**Constraints**: 
- Zero infrastructure imports (no Entity Framework, no HTTP, no Redis, no file I/O)
- Deterministic: identical inputs ‚Üí byte-identical outputs
- Immutable: all result objects must be immutable after construction
- Idempotent: evaluation does not modify input metrics, profiles, or rules
- No side effects: pure functional orchestration logic

**Scale/Scope**: 
- Single evaluation execution per invocation (no batching)
- Maximum 1000 metrics per execution
- Maximum 100 evaluation rules per execution
- Maximum 50 profiles available for selection
- Fingerprint computation must complete in < 1ms

---

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

**Specification-Driven Development**: ‚úÖ
- [x] Feature defined through explicit specification (spec.md with 4 user stories, 12 FRs, 8 success criteria)
- [x] Specification version-controlled and precedes all implementation
- [x] Implementation will follow specification requirements precisely

**Domain-Driven Design**: ‚úÖ
- [x] Orchestration layer works with domain models, not infrastructure concerns
- [x] Core logic expressed in ubiquitous language: EvaluationResult, CompletenessReport, Violation
- [x] Domain ports abstraction ensures no tight coupling to specific domain implementations
- [x] No domain rules redefined or extended; only orchestrated

**Clean Architecture**: ‚úÖ
- [x] Application layer (orchestration) depends only on Domain ports, not infrastructure
- [x] Direction of dependency: Application ‚Üí Domain Ports (inward, clean)
- [x] External systems (Metrics, Profile, Evaluation) accessed only through domain-defined ports
- [x] No infrastructure imports or infrastructure-layer logic in application package

**Layered Phase Independence**: ‚úÖ
- [x] Clear phase boundary: specification ‚Üí orchestration ‚Üí (infrastructure adapters separate)
- [x] Orchestration communicates with domains through serializable, engine-agnostic interfaces
- [x] Changes to domain internals do not require orchestration changes (interface contract maintained)

**Determinism & Reproducibility**: ‚úÖ
- [x] Identical inputs (metrics, profile, rules) produce identical outputs
- [x] Non-deterministic factors (timestamps) are inputs, not generated internally
- [x] All rule evaluation order deterministic (sorted by rule ID)
- [x] Fingerprint algorithm deterministic (SHA256 or equivalent, seeded, not randomized)

**Engine-Agnostic Abstraction**: ‚úÖ
- [x] Orchestration works with domain models, not engine-specific formats
- [x] Results normalized into domain EvaluationResult (metrics domain, evaluation domain, profile domain)
- [x] No engine data structures leak into result objects

**Evolution-Friendly Design**: ‚úÖ
- [x] New rule types added without orchestration changes (rules are domain inputs)
- [x] New metric types supported without orchestration changes (metrics are domain inputs)
- [x] Profile resolution strategy changes do not break orchestration (interface-based)
- [x] Outcome determination rules extensible through evaluation domain

---

## Project Structure

### Documentation (this feature)

```text
specs/002-evaluate-performance/
‚îú‚îÄ‚îÄ spec.md                      # Feature specification (COMPLETE)
‚îú‚îÄ‚îÄ plan.md                      # This file
‚îú‚îÄ‚îÄ research.md                  # Phase 0 (research findings)
‚îú‚îÄ‚îÄ data-model.md                # Phase 1 (domain model details)
‚îú‚îÄ‚îÄ contracts/                   # Phase 1 (API contracts)
‚îÇ   ‚îú‚îÄ‚îÄ orchestration-contracts.md
‚îÇ   ‚îî‚îÄ‚îÄ result-structures.md
‚îú‚îÄ‚îÄ quickstart.md                # Phase 1 (implementation quickstart)
‚îú‚îÄ‚îÄ checklists/
‚îÇ   ‚îî‚îÄ‚îÄ requirements.md          # Quality validation (COMPLETE)
‚îî‚îÄ‚îÄ tasks.md                     # Phase 2 (implementation tasks)
```

### Source Code

```text
src/
‚îú‚îÄ‚îÄ PerformanceEngine.Evaluation.Domain/          # Existing (not modified)
‚îú‚îÄ‚îÄ PerformanceEngine.Metrics.Domain/             # Existing (not modified)
‚îú‚îÄ‚îÄ PerformanceEngine.Profile.Domain/             # Existing (not modified)
‚îî‚îÄ‚îÄ PerformanceEngine.Application/                # NEW: Orchestration layer
    ‚îú‚îÄ‚îÄ PerformanceEngine.Application.csproj
    ‚îú‚îÄ‚îÄ Ports/                                    # Domain port abstractions
    ‚îÇ   ‚îú‚îÄ‚îÄ IMetricsProvider.cs                   # Receive metrics from Metrics Domain
    ‚îÇ   ‚îú‚îÄ‚îÄ IProfileResolver.cs                   # Resolve profiles from Profile Domain
    ‚îÇ   ‚îî‚îÄ‚îÄ IEvaluationRulesProvider.cs           # Receive rules from Evaluation Domain
    ‚îú‚îÄ‚îÄ Orchestration/                            # Core orchestration logic
    ‚îÇ   ‚îú‚îÄ‚îÄ EvaluatePerformanceUseCase.cs         # Main orchestration entry point
    ‚îÇ   ‚îú‚îÄ‚îÄ EvaluationOrchestrator.cs             # Orchestration workflow
    ‚îÇ   ‚îú‚îÄ‚îÄ CompletenessAssessor.cs               # Assess metric availability
    ‚îÇ   ‚îú‚îÄ‚îÄ RuleEvaluationCoordinator.cs          # Coordinate rule application
    ‚îÇ   ‚îú‚îÄ‚îÄ OutcomeAggregator.cs                  # Aggregate rule outcomes
    ‚îÇ   ‚îî‚îÄ‚îÄ ResultConstructor.cs                  # Build immutable EvaluationResult
    ‚îú‚îÄ‚îÄ Models/                                   # Application-level models
    ‚îÇ   ‚îú‚îÄ‚îÄ EvaluationResult.cs                   # Immutable result (orchestration view)
    ‚îÇ   ‚îú‚îÄ‚îÄ CompletenessReport.cs                 # Data availability report
    ‚îÇ   ‚îú‚îÄ‚îÄ Violation.cs                          # Rule violation details
    ‚îÇ   ‚îú‚îÄ‚îÄ ExecutionMetadata.cs                  # Traceability information
    ‚îÇ   ‚îî‚îÄ‚îÄ EvaluationContext.cs                  # Internal orchestration context
    ‚îî‚îÄ‚îÄ Services/                                 # Helper services
        ‚îî‚îÄ‚îÄ DeterministicFingerprintGenerator.cs  # Generate data fingerprints

tests/
‚îú‚îÄ‚îÄ PerformanceEngine.Application.Tests/
    ‚îú‚îÄ‚îÄ Integration/
    ‚îÇ   ‚îú‚îÄ‚îÄ EvaluatePerformanceUseCaseTests.cs    # End-to-end orchestration tests
    ‚îÇ   ‚îú‚îÄ‚îÄ DeterminismTests.cs                   # Idempotency and reproducibility
    ‚îÇ   ‚îî‚îÄ‚îÄ PartialMetricsTests.cs                # Partial data handling
    ‚îú‚îÄ‚îÄ Unit/
    ‚îÇ   ‚îú‚îÄ‚îÄ CompletenessAssessorTests.cs
    ‚îÇ   ‚îú‚îÄ‚îÄ RuleEvaluationCoordinatorTests.cs
    ‚îÇ   ‚îú‚îÄ‚îÄ OutcomeAggregatorTests.cs
    ‚îÇ   ‚îú‚îÄ‚îÄ ResultConstructorTests.cs
    ‚îÇ   ‚îî‚îÄ‚îÄ DeterministicFingerprintGeneratorTests.cs
    ‚îî‚îÄ‚îÄ Fixtures/
        ‚îú‚îÄ‚îÄ MetricsTestData.cs
        ‚îú‚îÄ‚îÄ ProfileTestData.cs
        ‚îî‚îÄ‚îÄ EvaluationRulesTestData.cs
```

**Structure Decision**: Single library package `PerformanceEngine.Application` containing orchestration logic. This is positioned as the application layer that will eventually be wrapped by adapters (HTTP, CLI, etc.) in infrastructure packages. The design maintains clear separation: orchestration concerns here, domain concerns in existing domain packages.

---

## High-Level Architecture

### Responsibility Boundaries

**Evaluate Performance Use Case (This Plan)** ‚Äî Orchestration Only
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  APPLICATION LAYER: Orchestration                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇ Purpose: Coordinate Metrics ‚Üí Profile ‚Üí Rules ‚Üí Result     ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Resolve profile configuration                             ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Assess completeness (which metrics available)             ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Order rules deterministically                             ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Aggregate outcomes                                        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Generate immutable EvaluationResult                        ‚îÇ
‚îÇ  ‚îÇ                                                             ‚îÇ
‚îÇ  ‚îÇ NOT Responsible For:                                        ‚îÇ
‚îÇ  ‚îÇ ‚úó Calculating metrics (delegated to Metrics Domain)         ‚îÇ
‚îÇ  ‚îÇ ‚úó Evaluating rules in detail (delegated to Eval Domain)    ‚îÇ
‚îÇ  ‚îÇ ‚úó Storing results (deferred to infrastructure)             ‚îÇ
‚îÇ  ‚îÇ ‚úó Comparing to baseline (out of scope)                     ‚îÇ
‚îÇ  ‚îÇ ‚úó Integrating with external systems (out of scope)         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì uses domain ports (abstraction) ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DOMAIN LAYER: Existing Domains (NOT Modified Here)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Üô             ‚Üô                ‚Üô                                ‚îÇ
‚îÇ Metrics       Profile           Evaluation                       ‚îÇ
‚îÇ Domain        Domain            Domain                           ‚îÇ
‚îÇ ‚Ä¢ Sample      ‚Ä¢ ProfileConf     ‚Ä¢ Rule                          ‚îÇ
‚îÇ ‚Ä¢ Metric      ‚Ä¢ Threshold       ‚Ä¢ Outcome enum                  ‚îÇ
‚îÇ ‚Ä¢ Latency     ‚Ä¢ Selector        ‚Ä¢ Violation                     ‚îÇ
‚îÇ ‚Ä¢ Aggregation ‚Ä¢ Rules ref       ‚Ä¢ RuleEvaluation                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì adapted from infrastructure ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INFRASTRUCTURE LAYER: External Adapters                        ‚îÇ
‚îÇ  (HTTP API, Event Sourcing, Database, File I/O)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Dependency Direction
```
Infrastructure (HTTP, DB, Events)
    ‚Üë
    ‚îÇ implements
    ‚îÇ
Application (Orchestration) ‚Üí ports ‚Üê Domain (Metrics, Profile, Eval)
                    ‚îÇ
                    ‚îî‚îÄ imports ONLY (inward dependency, clean)
```

### Key Interaction Points

**With Metrics Domain** (IMetricsProvider port):
- **Input**: Collection of `Sample` objects (immutable, engine-agnostic)
- **Usage**: Query available metrics and their values for completeness assessment
- **Responsibility boundary**: Orchestration does not calculate; uses provided samples as-is
- **Determinism**: Metrics are inputs, not generated; ordering deterministic

**With Profile Domain** (IProfileResolver port):
- **Input**: Requested profile identifier, available profiles
- **Usage**: Resolve which configuration/thresholds apply to this evaluation
- **Responsibility boundary**: Orchestration does not define profiles; uses provided configuration
- **Determinism**: Profile resolution is deterministic given same ID and available profiles

**With Evaluation Domain** (IEvaluationRulesProvider port):
- **Input**: Set of evaluation rules with metric dependencies
- **Usage**: Apply rules to metrics in deterministic order, collect violations
- **Responsibility boundary**: Orchestration does not implement rule logic; delegates to domain
- **Determinism**: Rule order deterministic (sorted by rule ID); rule evaluation is pure (same inputs ‚Üí same violations)

---

## Input and Output Contracts

### Inputs (Conceptual)

The orchestration receives four distinct inputs:

#### 1. Collected Metrics (from Metrics Domain)
```
Type: IReadOnlyCollection<Sample>
Content:
  - Immutable Sample objects (metric name, value, unit, timestamp)
  - May be empty or partial (missing expected metrics)
  - No modifications during orchestration
  
Contract: Metrics are read-only references; orchestration may iterate but not mutate
```

#### 2. Execution Context (calling code)
```
Type: ExecutionContext (value object)
Content:
  - Execution ID (for traceability)
  - Execution timestamp (when evaluation occurs)
  - Environment/metadata (not used for rules, only metadata)
  
Contract: Context is input; does not affect rule outcomes, only metadata
```

#### 3. Available Profiles (from Profile Domain)
```
Type: IReadOnlyCollection<ProfileConfiguration>
Content:
  - Profile ID, name, thresholds, rule references
  - Immutable configuration objects
  
Contract: Profiles are read-only; orchestration selects but does not modify
```

#### 4. Evaluation Rules (from Evaluation Domain)
```
Type: IReadOnlyCollection<EvaluationRule>
Content:
  - Rule ID, name, metric dependencies, rule logic (abstracted)
  - Severity level (critical ‚Üí FAIL, non-critical ‚Üí WARN)
  
Contract: Rules are read-only; orchestration applies in deterministic order
```

### Output Contract

#### EvaluationResult (Immutable, Primary Output)
```csharp
public sealed class EvaluationResult
{
    // Outcome determination
    public Outcome Outcome { get; }                      // PASS | WARN | FAIL | INCONCLUSIVE
    
    // Detailed violation information
    public IReadOnlyList<Violation> Violations { get; } // What failed and why
    
    // Traceability and metadata
    public ExecutionMetadata Metadata { get; }           // Profile applied, thresholds used, timestamps
    
    // Data transparency
    public CompletenessReport Completeness { get; }      // Which metrics available, which missing
    
    // Data integrity
    public string DataFingerprint { get; }               // SHA256 of actual collected data
}
```

#### Outcome Enum
```
PASS         ‚Üí All evaluation rules satisfied; no violations
WARN         ‚Üí One or more non-critical rules failed (incomplete requirements)
FAIL         ‚Üí One or more critical rules failed (requirements not met)
INCONCLUSIVE ‚Üí Insufficient data (>50% metrics missing) or conflicting rule results
```

#### Violation Details
```csharp
public sealed class Violation
{
    public string RuleId { get; }              // Which rule failed
    public string RuleName { get; }            
    public double ExpectedThreshold { get; }  // What was expected
    public double ActualValue { get; }         // What was measured
    public string AffectedMetricName { get; }  // Which metric caused failure
    public SeverityLevel Severity { get; }     // Critical ‚Üí FAIL, Non-critical ‚Üí WARN
}
```

#### CompletenessReport
```csharp
public sealed class CompletenessReport
{
    public int MetricsProvidedCount { get; }   // How many metrics available
    public int MetricsExpectedCount { get; }   // How many expected
    public double Completeness { get; }        // 0.0 ‚Üí 1.0 (percentage)
    public IReadOnlyList<string> MissingMetrics { get; } // Which specific metrics missing
    public IReadOnlyList<string> UnevaluatedRules { get; } // Rules skipped due to missing data
}
```

### Immutability & Determinism Guarantees

1. **Immutability**: EvaluationResult and all nested objects are immutable after construction (sealed records, no setters)
2. **Idempotency**: Same inputs (metrics, profile, rules) executed twice produce byte-identical EvaluationResult
3. **Fingerprinting**: DataFingerprint computed from actual collected metrics (not expected metrics), deterministic (SHA256 with seeded order)
4. **No Side Effects**: Inputs are never modified; evaluation is pure functional

---

## Orchestration Flow

### Step-by-Step Workflow

```
INPUTS: metrics, executionContext, availableProfiles, evaluationRules
   ‚Üì
1. VALIDATE INPUTS
   ‚îî‚îÄ Fail if no available profiles
   ‚îî‚îÄ Fail if no evaluation rules
   ‚îî‚îÄ Metrics can be empty (partial data allowed)
   ‚Üì
2. RESOLVE PROFILE
   ‚îî‚îÄ Select profile from availableProfiles (determined by context or config)
   ‚îî‚îÄ Extract: thresholds, rule references, severity definitions
   ‚îî‚îÄ If profile not found ‚Üí FAIL with clear error
   ‚Üì
3. FILTER RULES FOR PROFILE
   ‚îî‚îÄ Only evaluate rules referenced by selected profile
   ‚îî‚îÄ Sort rules deterministically by rule ID (ASCII sort)
   ‚Üì
4. ASSESS COMPLETENESS
   ‚îî‚îÄ For each rule, check if all required metrics are available in collection
   ‚îî‚îÄ Count: metricsAvailable, metricsExpected, missingMetrics
   ‚îî‚îÄ Determine completeness percentage
   ‚îî‚îÄ Identify unevaluated rules (rules missing required metrics)
   ‚Üì
5. EVALUATE RULES (DETERMINISTIC ORDER)
   ‚îú‚îÄ For each rule in sorted order:
   ‚îÇ  ‚îú‚îÄ If required metrics missing ‚Üí SKIP (mark as unevaluated)
   ‚îÇ  ‚îú‚îÄ If metrics available ‚Üí DELEGATE TO EVALUATION DOMAIN
   ‚îÇ  ‚îÇ  ‚îî‚îÄ Domain evaluates rule, returns: passed OR violation details
   ‚îÇ  ‚îú‚îÄ Collect: all violations with rule ID, threshold, actual value, affected metric, severity
   ‚îî‚îÄ Sort violations by rule ID for deterministic output
   ‚Üì
6. AGGREGATE OUTCOME
   ‚îú‚îÄ If any critical rule violated ‚Üí FAIL
   ‚îú‚îÄ Else if any non-critical rule violated ‚Üí WARN
   ‚îú‚îÄ Else if completeness < threshold (e.g., 50%) ‚Üí INCONCLUSIVE
   ‚îú‚îÄ Else if all rules satisfied AND completeness sufficient ‚Üí PASS
   ‚îî‚îÄ Sort outcome determination rules in clear precedence
   ‚Üì
7. GENERATE FINGERPRINT
   ‚îî‚îÄ Collect actual metric values in deterministic order (sorted by metric name)
   ‚îî‚îÄ Create string representation: "metric1=value1|metric2=value2|..."
   ‚îî‚îÄ Compute SHA256 hash of string (seeded, not randomized)
   ‚Üì
8. CONSTRUCT IMMUTABLE RESULT
   ‚îî‚îÄ Build EvaluationResult:
      ‚îú‚îÄ Outcome (from step 6)
      ‚îú‚îÄ Violations (sorted by rule ID from step 5)
      ‚îú‚îÄ CompletenessReport (from step 4)
      ‚îú‚îÄ ExecutionMetadata (from inputs + profile selected)
      ‚îî‚îÄ DataFingerprint (from step 7)
   ‚îî‚îÄ Result is sealed/immutable; no further modifications
   ‚Üì
OUTPUT: EvaluationResult (immutable, deterministic, traceable)
```

### Orchestration Responsibilities (What This Layer Does)

- ‚úÖ Select profile based on execution context
- ‚úÖ Determine rule evaluation order (deterministically)
- ‚úÖ Assess which metrics are available vs. expected
- ‚úÖ Orchestrate calls to Evaluation Domain rule evaluation (order, error handling)
- ‚úÖ Collect violations and violations details
- ‚úÖ Aggregate rules outcomes into single Outcome
- ‚úÖ Generate deterministic fingerprint of actual data
- ‚úÖ Construct immutable result object
- ‚úÖ Provide traceability (which profile, which data, which rules evaluated)

### Orchestration Non-Responsibilities (What Domains Do)

- ‚ùå Calculate metrics (Metrics Domain)
- ‚ùå Implement rule evaluation logic (Evaluation Domain)
- ‚ùå Store or persist results (Infrastructure)
- ‚ùå Compare to baselines (Out of scope)
- ‚ùå Determine CI/CD exit codes (Out of scope)

---

## Determinism and Idempotency Guarantees

### How Deterministic Ordering is Ensured

#### Rule Evaluation Order
- **Rule Collection**: Collect all rules to evaluate (filtered by profile)
- **Sorting**: Sort by rule ID using **string comparison (ASCII order)** ‚Äî stable, language-independent
- **Iteration**: Evaluate in sorted order, always
- **Result**: Same rules, same order, every execution

#### Metric Ordering (for fingerprint and completeness)
- **Metric Collection**: Collect all provided metrics
- **Sorting**: Sort by metric name (ASCII order) before processing
- **Aggregation**: Aggregate (count, check existence) in sorted order
- **Result**: Same metrics, same order, every execution

#### Violation Ordering
- **Collection**: Collect all violations as rules are evaluated
- **Sorting**: Sort by rule ID (same as rule evaluation order)
- **Result**: Same violations, same order, every execution

### How Byte-Identical Output is Achieved

#### Immutable Result Objects
- All result objects are `sealed records` or immutable classes
- No setters; properties set only during construction
- No mutable collections; only `IReadOnlyList` exposed
- No `DateTime.Now` or random values used; all timestamps are inputs

#### Deterministic Fingerprint
- **Input**: Collected metrics (provided, not generated)
- **Ordering**: Sort metrics by name (stable)
- **Serialization**: Deterministic string format: `metric1=value1|metric2=value2|...`
- **Hashing**: SHA256 with fixed seed (not randomized)
- **Output**: Same hash string every execution with same metrics

#### Deterministic Outcome Aggregation
- **Rule Application**: Evaluate in sorted order; collect violations
- **Outcome Rules** (deterministic precedence):
  1. If any critical violation ‚Üí FAIL
  2. Else if any non-critical violation ‚Üí WARN
  3. Else if completeness < 50% ‚Üí INCONCLUSIVE
  4. Else ‚Üí PASS
- **Result**: Outcome determined by rules, not by order or timing

### Idempotency Guarantee

**Idempotent Contract**: 
```
Execute(metrics‚ÇÅ, profile‚ÇÅ, rules‚ÇÅ) = result‚ÇÅ
Execute(metrics‚ÇÅ, profile‚ÇÅ, rules‚ÇÅ) = result‚ÇÅ  (byte-identical)
Execute(metrics‚ÇÅ, profile‚ÇÅ, rules‚ÇÅ) = result‚ÇÅ  (byte-identical, N times)
```

**How Achieved**:
- All inputs are read-only (not modified during orchestration)
- No external I/O (no timestamps, no random, no database calls)
- All computations are pure functions (same input ‚Üí same output)
- Result objects immutable; no re-evaluation or caching

---

## Partial Metrics Handling Strategy

### Detection of Missing Metrics

```csharp
// Pseudo-code logic
foreach (rule in sortedRules)
{
    var requiredMetrics = rule.GetRequiredMetricNames();  // e.g., ["latency_p99", "cpu_usage"]
    var availableMetrics = collectedMetrics.Select(m => m.Name).ToSet();
    
    var missingForThisRule = requiredMetrics.Except(availableMetrics).ToList();
    
    if (missingForThisRule.Count > 0)
    {
        // Metrics missing for this rule
        unevaluatedRules.Add(rule.Id, missingForThisRule);
    }
}
```

### Handling Rules with Missing Dependencies

**Strategy**: Graceful Degradation (Not Crash)

1. **Detection Phase**: When assessing completeness, identify which rules have missing metrics
2. **Skipping**: Rules with missing metrics are **not evaluated** (not called to Evaluation Domain)
3. **Tracking**: Add rule ID to `unevaluatedRules` list in CompletenessReport
4. **Outcome Impact**: Skipped rules do not contribute to outcome (assumed satisfied, not included in violation count)
5. **Traceability**: CompletenessReport explicitly lists which rules could not be evaluated

### CompletenessReport Details

```
MetricsProvidedCount: 8       // How many samples collected
MetricsExpectedCount: 10      // How many samples typical
Completeness: 0.80            // 80% available
MissingMetrics: 
  - "memory_peak"
  - "gc_collection_time"
UnevaluatedRules:             // Rules not evaluated due to missing metrics
  - "memory_rule_1"
  - "gc_rule_2"
```

### Completeness Thresholds

- **Sufficient Data**: Completeness > 50% ‚Üí proceed with evaluation, report inconclusive if needed
- **Insufficient Data**: Completeness ‚â§ 50% ‚Üí outcome = INCONCLUSIVE (even if no violations in evaluated rules)
- **Extreme Case**: Zero metrics provided ‚Üí Completeness = 0%, outcome = INCONCLUSIVE, CompletenessReport lists all expected metrics as missing

### Example Scenarios

**Scenario 1**: 9 of 10 metrics available
```
‚úÖ 90% complete
‚Üí Evaluate all rules with available metrics
‚Üí Skip rules needing the 1 missing metric
‚Üí Outcome: PASS/WARN/FAIL (based on evaluated rules)
‚Üí CompletenessReport: Indicates 1 metric missing, 1 rule unevaluated
```

**Scenario 2**: 4 of 10 metrics available (40%)
```
‚ö†Ô∏è 40% complete (below 50% threshold)
‚Üí Evaluate all possible rules
‚Üí Outcome: INCONCLUSIVE (even if all evaluated rules pass)
‚Üí CompletenessReport: Indicates 6 metrics missing, completeness < threshold
```

**Scenario 3**: Zero metrics available
```
üî¥ 0% complete
‚Üí No rules can be evaluated
‚Üí All rules listed as unevaluated
‚Üí Outcome: INCONCLUSIVE
‚Üí CompletenessReport: All metrics listed as missing
```

---

## Error Handling Semantics

### Invalid Configuration (Fail Fast)

**Errors that MUST fail before evaluation begins**:

1. **No Profile Found**
   ```
   Input: profileId that doesn't exist in availableProfiles
   Action: Throw explicit error IMMEDIATELY
   Message: "Profile '{profileId}' not found in available profiles: [{list}]"
   ```

2. **No Evaluation Rules**
   ```
   Input: evaluationRules collection is empty
   Action: Throw explicit error IMMEDIATELY
   Message: "No evaluation rules provided; cannot evaluate"
   ```

3. **Invalid Rule Configuration**
   ```
   Input: Rule references metric that doesn't exist in metrics domain vocabulary
   Action: Throw explicit error IMMEDIATELY (domain validation)
   Message: "Rule '{ruleId}' references unknown metric '{metricName}'"
   ```

4. **Incompatible Threshold**
   ```
   Input: Profile threshold incompatible with metric unit or type
   Action: Throw explicit error IMMEDIATELY (domain validation)
   Message: "Profile threshold {value} {unit} incompatible with metric {metricName}"
   ```

### Missing Data (Graceful Degradation)

**Not errors; handled gracefully**:

- Partial metrics available ‚Üí Skip rules needing missing metrics, report in CompletenessReport
- No metrics available ‚Üí Evaluate no rules, outcome INCONCLUSIVE, report in CompletenessReport
- Some expected metrics missing ‚Üí Report in CompletenessReport, continue evaluation

### Rule Evaluation Errors (Captured, Not Crashing)

**If Evaluation Domain throws error during rule evaluation**:

1. **Catch exception** from domain rule evaluation
2. **Create error Violation entry** with:
   - RuleId, RuleName
   - ErrorMessage (from exception)
   - Severity: Critical (error treated as critical failure)
3. **Add to Violations list** (treat as violation, not infrastructure failure)
4. **Continue evaluation** of remaining rules
5. **Outcome**: At least FAIL (error counts as critical violation)

**Example**:
```
Evaluating rule "cpu_threshold_rule"
  ‚Üí Domain returns violation (threshold exceeded) ‚Üí Add to violations
Evaluating rule "memory_rule"
  ‚Üí Domain throws exception (metric value invalid) ‚Üí Create error violation, add to violations
Evaluating rule "latency_rule"
  ‚Üí Domain returns pass ‚Üí Continue
Final outcome: FAIL (due to violations + error)
```

---

## Non-Goals, Assumptions, and Open Questions

### Explicitly Deferred (Out of Scope)

1. **Metric Calculation**
   - Metrics are provided by caller or Metrics Domain; orchestration does not calculate
   - Deferred: How to calculate latency percentiles, aggregations, etc.

2. **Baseline Comparison**
   - Evaluating "improved" vs "degraded" compared to previous execution
   - Deferred: Baseline storage, comparison logic, historical data correlation

3. **Persistence**
   - Storing evaluation results, metrics, profiles, or execution history
   - Deferred: Database schema, cache invalidation, archival strategy

4. **Integration with External Systems**
   - Sending results to CI/CD, monitoring systems, Slack, PagerDuty, etc.
   - Deferred: Integration adapter design, webhook schemas

5. **CI/CD Exit Code Determination**
   - Determining if a process should exit with code 0 (success) vs 1 (failure)
   - Deferred: Business logic for exit code mapping, CI/CD semantics

6. **Profile Storage/Retrieval**
   - Persisting profile configurations, versioning, fetching
   - Deferred: Profile repository, versioning strategy, rollback

7. **Advanced Outcome Logic**
   - Weighted scoring, trend analysis, machine learning-based prediction
   - Deferred: Future enhancement, not in MVP

### Assumptions

1. **Metrics Already Collected**
   - Caller provides metrics already collected from execution
   - Orchestration receives finished metric collection, does not collect
   - Assumption: `IMetricsProvider` returns immutable, complete collection

2. **Profile Configuration Valid**
   - Profile selected is already valid and contains:
     - Referenced rules exist in Evaluation Domain
     - Thresholds are compatible with metric types
     - No circular dependencies or conflicts
   - Assumption: Profile resolved by Profile Domain before passing to orchestration

3. **Evaluation Rules Immutable**
   - Rules do not change during evaluation
   - Same rule evaluated twice produces same result (given same metrics)
   - Assumption: Rules are read-only, deterministic domain objects

4. **Evaluation Domain is Pure**
   - Evaluation Domain rule application is pure: same metrics + rule ‚Üí same violation result
   - No side effects during rule evaluation
   - Assumption: Domain layer follows pure functional semantics

5. **Standard Cryptography Available**
   - SHA256 or equivalent deterministic hashing function available
   - Hashing is seeded (not randomized)
   - Assumption: .NET 10 `System.Security.Cryptography` available

6. **Execution Environment Deterministic for Our Purposes**
   - Floating-point arithmetic produces same results across runs (within .NET platform)
   - No timing-dependent outcomes
   - Assumption: Orchestration does not depend on system clock or environment-specific behavior

7. **Maximum Scale Assumptions**
   - < 1000 metrics per execution
   - < 100 evaluation rules per execution
   - < 50 available profiles to select from
   - Assumption: Performance targets met within these bounds

### Open Questions for Stakeholder/Architect Review

1. **Profile Selection Logic**
   - Q: How is the profile determined from execution context?
   - A: [NEEDS CLARIFICATION - implementation decision required]
   - Options: 
     - By environment name (prod, staging, dev)?
     - By execution tags/labels?
     - By API parameter?
     - By last-successful profile?

2. **Fingerprint Algorithm**
   - Q: Should fingerprint include metric timestamps, or only values?
   - A: [NEEDS CLARIFICATION - specification decision]
   - Impact: Affects whether same metrics collected at different times have same fingerprint

3. **Completeness Threshold**
   - Q: The spec says "50% missing metrics" ‚Üí INCONCLUSIVE; is this the final threshold?
   - A: [NEEDS CLARIFICATION - specification decision]
   - Impact: Rules how partial data is handled

4. **Rule Evaluation Error Handling**
   - Q: If domain rule throws exception, should entire evaluation fail or just that rule?
   - A: [NEEDS CLARIFICATION - specified as "captured as violation" but needs test coverage]
   - Impact: Failure mode for corrupt/invalid rules

5. **Outcome Precedence**
   - Q: If evaluation produces BOTH WARN-level violations AND insufficient metrics, should outcome be WARN or INCONCLUSIVE?
   - A: [NEEDS CLARIFICATION - precedence rules for edge cases]
   - Current assumption: Insufficient completeness ‚Üí INCONCLUSIVE (overrides violations)

6. **Non-Critical vs Critical Rules**
   - Q: How does orchestration know which rules are critical vs non-critical?
   - A: Specified in rule metadata via domain port
   - Confirmation: Verify domain rule interface exposes `IsCritical` or `Severity` property

---

## Technology Decisions & Justifications

### Why Application Layer (Not Domain Layer)

**Decision**: Orchestration implemented as **Application Layer**, not extending existing domains

**Justification**:
- Domains (Metrics, Profile, Evaluation) are independently valuable
- Orchestration is a **workflow** that uses domains, not domain logic itself
- Clean Architecture principle: Application layer coordinates between domains
- Allows future orchestrations (e.g., multiple profiles, comparison evaluations) without modifying domains
- Easier to test orchestration separately from domain logic

**Alternative Rejected**: Putting orchestration in Evaluation Domain would couple domains together, violating separation of concerns

### Why No Infrastructure in This Layer

**Decision**: Zero infrastructure imports (no Entity Framework, HTTP, Redis, files)

**Justification**:
- Ensures orchestration logic is reusable across infrastructure boundaries
- Simplifies testing (no mocks needed for databases or HTTP clients)
- Follows Clean Architecture principle: dependencies point inward only
- Allows multiple infrastructure implementations (HTTP API, gRPC, Event-driven, CLI) all using same orchestration

**Alternative Rejected**: Embedding infrastructure (e.g., caching, logging) would lock orchestration to specific infrastructure choice

### Why C# 13 (.NET 10 LTS)

**Decision**: Use C# 13 with .NET 10 LTS

**Justification**:
- Consistent with existing domain projects in this repository
- .NET 10 LTS: 3-year support window, production-ready
- C# 13: Record types enable immutable result objects elegantly
- Strong null-safety features (`#nullable enable`) prevent NullReferenceException errors
- Cross-platform (.NET 10 runs on Linux, Windows, macOS)

---

## Testing Strategy

### Unit Tests (Orchestration Logic)

**What to test**:
- Rule sorting determinism (same rules always same order)
- Completeness assessment (correct counting of available metrics)
- Outcome aggregation (correct precedence: FAIL > WARN > INCONCLUSIVE > PASS)
- Violation collection (all violations captured, none lost)
- Fingerprint generation (same metrics ‚Üí same hash, different ‚Üí different)
- Immutability (result objects cannot be modified after construction)

**Test Framework**: xUnit with FluentAssertions

**Example tests**:
- `DeterministicRuleOrdering_SameRulesAlwaysSameOrder`
- `CompletenessAssessment_9Of10Metrics_Returns90Percent`
- `CompletenessThreshold_40PercentComplete_OutcomeInconclusive`
- `FingerprintDeterminism_SameMetricsProduceSameHash`
- `ViolationCollection_AllRulesEvaluated_AllViolationsCaptured`

### Integration Tests (With Domain Test Doubles)

**What to test**:
- Full orchestration flow (inputs ‚Üí result)
- Interaction with domain ports (calls domain in correct order, handles returns)
- Error handling (invalid config caught before evaluation)
- Partial metrics handling (skip rules, report completeness)

**Setup**: Use test doubles for domain ports (not real domain implementation)

**Example scenarios**:
- `FullOrchestration_AllRulesPass_OutcomePass`
- `FullOrchestration_SomeRulesFail_OutcomeFail`
- `FullOrchestration_50PercentMetricsMissing_OutcomeInconclusive`
- `InvalidProfile_NotFound_ThrowsBeforeEvaluation`
- `RuleEvaluationError_CaughtAndCaptured_NotCrash`

### Determinism Tests (Reproducibility Validation)

**What to test**:
- Execute same scenario 10 times, all results byte-identical
- Change one metric value, fingerprint changes
- Restore metric value, fingerprint matches original

**Example**:
```csharp
[Fact]
public void OrchestrationIdempotency_SameInputProducesByteIdenticalOutput()
{
    // Arrange
    var metrics = CreateTestMetrics();
    var profile = CreateTestProfile();
    var rules = CreateTestRules();
    
    // Act
    var result1 = orchestrator.Evaluate(metrics, profile, rules);
    var result2 = orchestrator.Evaluate(metrics, profile, rules);
    var result3 = orchestrator.Evaluate(metrics, profile, rules);
    
    // Assert
    Assert.Equal(SerializeToJson(result1), SerializeToJson(result2));
    Assert.Equal(SerializeToJson(result1), SerializeToJson(result3));
}
```

---

## Summary Table

| Aspect | Details |
|--------|---------|
| **Layer** | Application (Clean Architecture) |
| **Responsibility** | Orchestration only (coordinate, not calculate) |
| **Input** | Metrics, profile, rules, execution context |
| **Output** | Immutable EvaluationResult |
| **Determinism** | Deterministic sorting, immutable objects, seeded fingerprints |
| **Idempotency** | Same input ‚Üí byte-identical output, always |
| **Partial Data** | Gracefully degrade (skip rules, report completeness) |
| **Error Handling** | Fail fast on invalid config; capture domain errors as violations |
| **Dependencies** | Domain ports only (inward dependency, clean architecture) |
| **No Infrastructure** | No persistence, HTTP, Redis, file I/O in this layer |
| **Testing** | Unit (logic), Integration (domains), Determinism (reproducibility) |
| **Scale** | < 1000 metrics, < 100 rules, < 50 profiles per execution |
| **Performance** | Orchestration < 50ms overhead for typical workloads |

---

## Next Steps (Phase 1: Design)

This plan will be followed by Phase 1 activities:

1. **data-model.md**: Define orchestration domain models (EvaluationResult, Violation, etc.)
2. **contracts/**: API contracts for domain ports (IMetricsProvider, IProfileResolver, IEvaluationRulesProvider)
3. **quickstart.md**: Step-by-step guide to implementing the use case
4. **tasks.md** (Phase 2): Break plan into concrete implementation tasks

---

**Plan Status**: ‚úÖ READY FOR PHASE 1 DESIGN
